{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNGeBCPsJI/y3OoWj8T9Vyo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# DenseNet\n","upload transposed2.Rds"],"metadata":{"id":"7NhhpxA8KJQF"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DwWQXInRuh8D","executionInfo":{"status":"ok","timestamp":1699576370257,"user_tz":600,"elapsed":56009,"user":{"displayName":"Joshua Burkhart","userId":"09471866206613143181"}},"outputId":"63946a4e-6095-4667-b328-8abafd6bdc96"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0, Train Acc: 0.15625\n","Epoch: 1, Train Acc: 0.2569444444444444\n","Epoch: 2, Train Acc: 0.2673611111111111\n","Epoch: 3, Train Acc: 0.2604166666666667\n","Epoch: 4, Train Acc: 0.2881944444444444\n","Epoch: 5, Train Acc: 0.28125\n","Epoch: 6, Train Acc: 0.3506944444444444\n","Epoch: 7, Train Acc: 0.2986111111111111\n","Epoch: 8, Train Acc: 0.4340277777777778\n","Epoch: 9, Train Acc: 0.4722222222222222\n","Epoch: 10, Train Acc: 0.3506944444444444\n","Epoch: 11, Train Acc: 0.4652777777777778\n","Epoch: 12, Train Acc: 0.5659722222222222\n","Epoch: 13, Train Acc: 0.5277777777777778\n","Epoch: 14, Train Acc: 0.6319444444444444\n","Epoch: 15, Train Acc: 0.6736111111111112\n","Epoch: 16, Train Acc: 0.7673611111111112\n","Epoch: 17, Train Acc: 0.8055555555555556\n","Epoch: 18, Train Acc: 0.75\n","Epoch: 19, Train Acc: 0.6736111111111112\n","Epoch: 20, Train Acc: 0.8229166666666666\n","Epoch: 21, Train Acc: 0.8333333333333334\n","Epoch: 22, Train Acc: 0.8472222222222222\n","Epoch: 23, Train Acc: 0.7777777777777778\n","Epoch: 24, Train Acc: 0.8263888888888888\n","Epoch: 25, Train Acc: 0.90625\n","Epoch: 26, Train Acc: 0.9513888888888888\n","Epoch: 27, Train Acc: 0.9618055555555556\n","Epoch: 28, Train Acc: 0.9618055555555556\n","Epoch: 29, Train Acc: 0.9618055555555556\n","Epoch: 30, Train Acc: 0.9513888888888888\n","Epoch: 31, Train Acc: 0.8229166666666666\n","Epoch: 32, Train Acc: 0.8645833333333334\n","Epoch: 33, Train Acc: 0.7222222222222222\n","Epoch: 34, Train Acc: 0.8854166666666666\n","Epoch: 35, Train Acc: 0.8402777777777778\n","Epoch: 36, Train Acc: 0.9583333333333334\n","Epoch: 37, Train Acc: 0.9791666666666666\n","Epoch: 38, Train Acc: 0.9930555555555556\n","Epoch: 39, Train Acc: 1.0\n"]}],"source":["import rpy2.robjects as ro\n","from rpy2.robjects import r\n","import torch\n","from rpy2.robjects import pandas2ri\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import models\n","from torch import nn\n","from rpy2.robjects.conversion import localconverter\n","\n","torch.manual_seed(88888888)\n","\n","DATA_PATH = 'transposed2.Rds' #'/home/jgburk/PycharmProjects/IntroductionToDeepLearning/transposed.Rds'\n","NUM_FEATURES = 1000\n","NUM_CLASSES = 7\n","NUM_TRAIN_IT = 1000\n","\n","BATCH_SIZE = 32\n","\n","\n","class PandasDataset(Dataset):\n","\n","    def __init__(self, rds_fn):\n","      with localconverter(ro.default_converter + pandas2ri.converter):\n","        df = ro.conversion.rpy2py(r.readRDS(rds_fn))\n","\n","        features = df.iloc[:, 0:NUM_FEATURES].values\n","        targets = df['sample_type'].values\n","\n","        self.x = torch.tensor(features)\n","        self.y = torch.tensor(targets - 1)\n","\n","    def __len__(self):\n","        return len(self.y)\n","\n","    def __getitem__(self, i):\n","        return self.x[i], self.y[i]\n","\n","\n","def train(loader, dv):\n","    model.train()\n","\n","    correct = 0\n","    for batch in loader:  # Iterate in batches over the training dataset.\n","        x = batch[0].reshape(BATCH_SIZE, 4, 10, 25).to(dv)\n","        y = batch[1].to(dv)\n","        out = model(x.float())  # Perform a single forward pass.\n","        y = torch.squeeze(y)\n","        loss = criterion(out, y.long())  # Compute the loss.\n","        loss.backward()  # Derive gradients.\n","        optimizer.step()  # Update parameters based on gradients.\n","        optimizer.zero_grad()  # Clear gradients.\n","        pred = out.argmax(dim=1)  # Use the class with highest probability.\n","        correct += int((pred == y).sum())  # Check against ground-truth labels.\n","    return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n","\n","\n","rds_dataset = PandasDataset(DATA_PATH)\n","training_loader = DataLoader(rds_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","\n","model = models.DenseNet(num_classes=NUM_CLASSES)\n","conv0 = model.features.conv0\n","model.features.conv0 = nn.Conv2d(4, out_channels=conv0.out_channels, kernel_size=conv0.kernel_size,\n","                                 stride=conv0.stride, padding=conv0.padding, bias=conv0.bias)\n","model.features.transition1.pool = nn.AvgPool2d(kernel_size=2, stride=2, padding=1)\n","model.features.transition2.pool = nn.AvgPool2d(kernel_size=2, stride=2, padding=1)\n","model.features.transition3.pool = nn.AvgPool2d(kernel_size=2, stride=2, padding=1)\n","device = torch.device('cuda')\n","model.to(device)\n","\n","optimizer = torch.optim.AdamW(model.parameters())\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","for epoch in range(NUM_TRAIN_IT):\n","    train(training_loader, device)\n","    train_acc = train(training_loader, device)\n","    print(f'Epoch: {epoch}, Train Acc: {train_acc}')\n","    if train_acc == 1.0:\n","        break"]}]}