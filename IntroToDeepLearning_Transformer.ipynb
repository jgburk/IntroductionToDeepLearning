{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPU65plFJUfV5/KdXAUa3sJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Transformer\n","upload sim_seq_1_train_sequences2.txt"],"metadata":{"id":"ppy9v244J5Vc"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Ry_dX0L2Kbo","outputId":"45ea6768-a123-4db9-98e6-2f46dcd5a6bd"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-1-59ecca75916f>:58: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n","  training_data = numpy.genfromtxt(training_data_fn, delimiter=\"\\t\", dtype=None,\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n","  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 0, Loss: 10.697022438049316\n","Epoch: 1, Loss: 9.228446006774902\n","Epoch: 2, Loss: 8.176187515258789\n","Epoch: 3, Loss: 7.170403480529785\n","Epoch: 4, Loss: 6.189487934112549\n","Epoch: 5, Loss: 5.233256816864014\n","Epoch: 6, Loss: 4.326925277709961\n","Epoch: 7, Loss: 3.3854193687438965\n","Epoch: 8, Loss: 2.5347325801849365\n","Epoch: 9, Loss: 1.972529411315918\n","Epoch: 10, Loss: 1.6806044578552246\n","Epoch: 11, Loss: 1.7128338813781738\n","Epoch: 12, Loss: 1.9110195636749268\n","Epoch: 13, Loss: 2.1989264488220215\n","Epoch: 14, Loss: 2.274672269821167\n","Epoch: 15, Loss: 2.08693790435791\n","Epoch: 16, Loss: 1.608951210975647\n","Epoch: 17, Loss: 1.9913123846054077\n","Epoch: 18, Loss: 1.960890531539917\n","Epoch: 19, Loss: 1.6551308631896973\n","Epoch: 20, Loss: 1.6070233583450317\n","Epoch: 21, Loss: 1.7863589525222778\n","Epoch: 22, Loss: 1.827773094177246\n","Epoch: 23, Loss: 1.7122563123703003\n","Epoch: 24, Loss: 1.6502354145050049\n","Epoch: 25, Loss: 1.6185617446899414\n","Epoch: 26, Loss: 1.68135666847229\n","Epoch: 27, Loss: 1.6948930025100708\n","Epoch: 28, Loss: 1.7042839527130127\n","Epoch: 29, Loss: 1.6913946866989136\n","Epoch: 30, Loss: 1.6656113862991333\n","Epoch: 31, Loss: 1.6727465391159058\n","Epoch: 32, Loss: 1.6816149950027466\n","Epoch: 33, Loss: 1.7078731060028076\n","Epoch: 34, Loss: 1.7058547735214233\n","Epoch: 35, Loss: 1.6788718700408936\n","Epoch: 36, Loss: 1.6361792087554932\n","Epoch: 37, Loss: 1.6818671226501465\n","Epoch: 38, Loss: 1.6501686573028564\n","Epoch: 39, Loss: 1.6341396570205688\n","Epoch: 40, Loss: 1.6515839099884033\n","Epoch: 41, Loss: 1.6557230949401855\n","Epoch: 42, Loss: 1.633530855178833\n","Epoch: 43, Loss: 1.6647460460662842\n","Epoch: 44, Loss: 1.6704819202423096\n","Epoch: 45, Loss: 1.6362379789352417\n","Epoch: 46, Loss: 1.6589967012405396\n","Epoch: 47, Loss: 1.6023025512695312\n","Epoch: 48, Loss: 1.6434657573699951\n","Epoch: 49, Loss: 1.6623533964157104\n","Epoch: 50, Loss: 1.6056981086730957\n","Epoch: 51, Loss: 1.6163182258605957\n","Epoch: 52, Loss: 1.648155927658081\n","Epoch: 53, Loss: 1.6416047811508179\n","Epoch: 54, Loss: 1.5851871967315674\n","Epoch: 55, Loss: 1.6631640195846558\n","Epoch: 56, Loss: 1.6545730829238892\n","Epoch: 57, Loss: 1.6160222291946411\n","Epoch: 58, Loss: 1.6351574659347534\n","Epoch: 59, Loss: 1.5766950845718384\n","Epoch: 60, Loss: 1.5913127660751343\n","Epoch: 61, Loss: 1.6016250848770142\n","Epoch: 62, Loss: 1.5687973499298096\n","Epoch: 63, Loss: 1.5948796272277832\n","Epoch: 64, Loss: 1.553076148033142\n","Epoch: 65, Loss: 1.5964748859405518\n","Epoch: 66, Loss: 1.5379431247711182\n","Epoch: 67, Loss: 1.5700290203094482\n","Epoch: 68, Loss: 1.5003188848495483\n","Epoch: 69, Loss: 1.5533854961395264\n","Epoch: 70, Loss: 1.5487397909164429\n","Epoch: 71, Loss: 1.5475577116012573\n","Epoch: 72, Loss: 1.5268954038619995\n","Epoch: 73, Loss: 1.4447575807571411\n","Epoch: 74, Loss: 1.48444402217865\n","Epoch: 75, Loss: 1.5294408798217773\n","Epoch: 76, Loss: 1.3849899768829346\n","Epoch: 77, Loss: 1.4462790489196777\n","Epoch: 78, Loss: 1.4193143844604492\n","Epoch: 79, Loss: 1.3358443975448608\n","Epoch: 80, Loss: 1.4299569129943848\n","Epoch: 81, Loss: 1.3058689832687378\n","Epoch: 82, Loss: 1.3746675252914429\n","Epoch: 83, Loss: 1.3251581192016602\n","Epoch: 84, Loss: 1.24113130569458\n","Epoch: 85, Loss: 1.3031500577926636\n","Epoch: 86, Loss: 1.3209099769592285\n","Epoch: 87, Loss: 1.2691359519958496\n","Epoch: 88, Loss: 1.232267141342163\n","Epoch: 89, Loss: 1.2394745349884033\n","Epoch: 90, Loss: 1.1949880123138428\n","Epoch: 91, Loss: 1.134585976600647\n","Epoch: 92, Loss: 1.1185799837112427\n","Epoch: 93, Loss: 1.2456780672073364\n","Epoch: 94, Loss: 1.1528249979019165\n","Epoch: 95, Loss: 1.0802556276321411\n","Epoch: 96, Loss: 1.0150933265686035\n","Epoch: 97, Loss: 1.0032790899276733\n","Epoch: 98, Loss: 1.0405224561691284\n","Epoch: 99, Loss: 1.0702197551727295\n","Epoch: 100, Loss: 1.1421465873718262\n","Epoch: 101, Loss: 0.9369714260101318\n","Epoch: 102, Loss: 0.9616274833679199\n","Epoch: 103, Loss: 0.9328934550285339\n","Epoch: 104, Loss: 1.066096544265747\n","Epoch: 105, Loss: 0.9009510278701782\n","Epoch: 106, Loss: 0.9810665249824524\n","Epoch: 107, Loss: 0.8867700099945068\n","Epoch: 108, Loss: 0.8705326914787292\n","Epoch: 109, Loss: 0.8850483894348145\n","Epoch: 110, Loss: 0.8162665367126465\n","Epoch: 111, Loss: 0.9039941430091858\n","Epoch: 112, Loss: 0.8270451426506042\n","Epoch: 113, Loss: 0.7753775119781494\n","Epoch: 114, Loss: 0.7084843516349792\n","Epoch: 115, Loss: 0.7290843725204468\n","Epoch: 116, Loss: 0.6986260414123535\n","Epoch: 117, Loss: 0.691153347492218\n","Epoch: 118, Loss: 0.7307934761047363\n","Epoch: 119, Loss: 0.7888907790184021\n","Epoch: 120, Loss: 0.6357817649841309\n","Epoch: 121, Loss: 0.6998830437660217\n","Epoch: 122, Loss: 0.5836683511734009\n","Epoch: 123, Loss: 0.6799623370170593\n","Epoch: 124, Loss: 0.5646237730979919\n","Epoch: 125, Loss: 0.7237979769706726\n","Epoch: 126, Loss: 0.5577229857444763\n","Epoch: 127, Loss: 0.7567386627197266\n","Epoch: 128, Loss: 0.5370954275131226\n","Epoch: 129, Loss: 0.7219911813735962\n","Epoch: 130, Loss: 0.6406732797622681\n","Epoch: 131, Loss: 0.5511288642883301\n","Epoch: 132, Loss: 0.7951449751853943\n","Epoch: 133, Loss: 0.6275054812431335\n","Epoch: 134, Loss: 0.5984362959861755\n","Epoch: 135, Loss: 0.802082896232605\n","Epoch: 136, Loss: 0.4823400378227234\n","Epoch: 137, Loss: 0.6821202635765076\n","Epoch: 138, Loss: 0.6808253526687622\n","Epoch: 139, Loss: 0.4896846413612366\n","Epoch: 140, Loss: 0.8243971467018127\n","Epoch: 141, Loss: 0.9404386878013611\n","Epoch: 142, Loss: 0.5619867444038391\n","Epoch: 143, Loss: 0.6250786781311035\n","Epoch: 144, Loss: 0.73117995262146\n","Epoch: 145, Loss: 0.510488748550415\n","Epoch: 146, Loss: 0.5354064106941223\n","Epoch: 147, Loss: 0.7812510132789612\n","Epoch: 148, Loss: 0.507769763469696\n","Epoch: 149, Loss: 0.43646132946014404\n","Epoch: 150, Loss: 0.4963000416755676\n","Epoch: 151, Loss: 0.4615475535392761\n","Epoch: 152, Loss: 0.38051390647888184\n","Epoch: 153, Loss: 0.4725151062011719\n","Epoch: 154, Loss: 0.4654736816883087\n","Epoch: 155, Loss: 0.4804941415786743\n","Epoch: 156, Loss: 0.44638946652412415\n","Epoch: 157, Loss: 0.5177075862884521\n","Epoch: 158, Loss: 0.4839361608028412\n","Epoch: 159, Loss: 0.44729849696159363\n","Epoch: 160, Loss: 0.38032352924346924\n","Epoch: 161, Loss: 0.4713085889816284\n","Epoch: 162, Loss: 0.5607932806015015\n","Epoch: 163, Loss: 0.40178123116493225\n","Epoch: 164, Loss: 0.512346625328064\n","Epoch: 165, Loss: 0.6152873039245605\n","Epoch: 166, Loss: 0.3942820727825165\n","Epoch: 167, Loss: 0.47946614027023315\n","Epoch: 168, Loss: 0.5635213255882263\n","Epoch: 169, Loss: 0.4317275881767273\n","Epoch: 170, Loss: 0.39195525646209717\n","Epoch: 171, Loss: 0.447261780500412\n","Epoch: 172, Loss: 0.44868049025535583\n","Epoch: 173, Loss: 0.3427967131137848\n","Epoch: 174, Loss: 0.44967955350875854\n","Epoch: 175, Loss: 0.48128899931907654\n","Epoch: 176, Loss: 0.3887808322906494\n","Epoch: 177, Loss: 0.40284106135368347\n","Epoch: 178, Loss: 0.36649665236473083\n","Epoch: 179, Loss: 0.38323771953582764\n","Epoch: 180, Loss: 0.37616294622421265\n","Epoch: 181, Loss: 0.41376835107803345\n","Epoch: 182, Loss: 0.4302786886692047\n","Epoch: 183, Loss: 0.41683268547058105\n","Epoch: 184, Loss: 0.4020955562591553\n","Epoch: 185, Loss: 0.40195998549461365\n","Epoch: 186, Loss: 0.3687613606452942\n","Epoch: 187, Loss: 0.398787260055542\n","Epoch: 188, Loss: 0.3687940537929535\n","Epoch: 189, Loss: 0.3677446246147156\n","Epoch: 190, Loss: 0.36127471923828125\n","Epoch: 191, Loss: 0.3423299789428711\n","Epoch: 192, Loss: 0.3283739387989044\n","Epoch: 193, Loss: 0.35683709383010864\n","Epoch: 194, Loss: 0.3584301769733429\n","Epoch: 195, Loss: 0.3377249240875244\n","Epoch: 196, Loss: 0.36525553464889526\n","Epoch: 197, Loss: 0.3912297189235687\n","Epoch: 198, Loss: 0.32668620347976685\n","Epoch: 199, Loss: 0.392191082239151\n","Epoch: 200, Loss: 0.32248589396476746\n","Epoch: 201, Loss: 0.39184269309043884\n","Epoch: 202, Loss: 0.39643508195877075\n","Epoch: 203, Loss: 0.3677786886692047\n","Epoch: 204, Loss: 0.32941436767578125\n","Epoch: 205, Loss: 0.40094855427742004\n","Epoch: 206, Loss: 0.3700661361217499\n","Epoch: 207, Loss: 0.32654130458831787\n","Epoch: 208, Loss: 0.3636413514614105\n","Epoch: 209, Loss: 0.3299206495285034\n","Epoch: 210, Loss: 0.31651216745376587\n","Epoch: 211, Loss: 0.37975189089775085\n","Epoch: 212, Loss: 0.3387180268764496\n","Epoch: 213, Loss: 0.36334702372550964\n","Epoch: 214, Loss: 0.3308212161064148\n","Epoch: 215, Loss: 0.33556708693504333\n","Epoch: 216, Loss: 0.3444876968860626\n","Epoch: 217, Loss: 0.46872544288635254\n","Epoch: 218, Loss: 0.3123185634613037\n","Epoch: 219, Loss: 0.36271393299102783\n","Epoch: 220, Loss: 0.37585899233818054\n","Epoch: 221, Loss: 0.3437154293060303\n","Epoch: 222, Loss: 0.32378676533699036\n","Epoch: 223, Loss: 0.2830125689506531\n","Epoch: 224, Loss: 0.3261328935623169\n","Epoch: 225, Loss: 0.28843405842781067\n","Epoch: 226, Loss: 0.3140075206756592\n","Epoch: 227, Loss: 0.3321853578090668\n","Epoch: 228, Loss: 0.2905060350894928\n","Epoch: 229, Loss: 0.2648112177848816\n","Epoch: 230, Loss: 0.3465156555175781\n","Epoch: 231, Loss: 0.3732503652572632\n","Epoch: 232, Loss: 0.29800716042518616\n","Epoch: 233, Loss: 0.30540600419044495\n","Epoch: 234, Loss: 0.3089434504508972\n","Epoch: 235, Loss: 0.28814399242401123\n","Epoch: 236, Loss: 0.2823998034000397\n","Epoch: 237, Loss: 0.3262130618095398\n","Epoch: 238, Loss: 0.2790776193141937\n","Epoch: 239, Loss: 0.3156975507736206\n","Epoch: 240, Loss: 0.3111976385116577\n","Epoch: 241, Loss: 0.31606969237327576\n","Epoch: 242, Loss: 0.319826602935791\n","Epoch: 243, Loss: 0.29572537541389465\n","Epoch: 244, Loss: 0.2718780040740967\n","Epoch: 245, Loss: 0.31496569514274597\n","Epoch: 246, Loss: 0.3619632422924042\n","Epoch: 247, Loss: 0.2732945382595062\n","Epoch: 248, Loss: 0.3017622232437134\n","Epoch: 249, Loss: 0.3485020101070404\n","Epoch: 250, Loss: 0.3382483422756195\n","Epoch: 251, Loss: 0.34729117155075073\n","Epoch: 252, Loss: 0.3095453977584839\n","Epoch: 253, Loss: 0.3490428030490875\n","Epoch: 254, Loss: 0.377848744392395\n","Epoch: 255, Loss: 0.342595636844635\n","Epoch: 256, Loss: 0.35900330543518066\n","Epoch: 257, Loss: 0.3283505141735077\n","Epoch: 258, Loss: 0.33470165729522705\n","Epoch: 259, Loss: 0.28283631801605225\n","Epoch: 260, Loss: 0.3099965751171112\n","Epoch: 261, Loss: 0.2656407654285431\n","Epoch: 262, Loss: 0.2636070251464844\n","Epoch: 263, Loss: 0.29327383637428284\n","Epoch: 264, Loss: 0.24937376379966736\n","Epoch: 265, Loss: 0.30782371759414673\n","Epoch: 266, Loss: 0.29627352952957153\n","Epoch: 267, Loss: 0.2557380795478821\n","Epoch: 268, Loss: 0.3244187831878662\n","Epoch: 269, Loss: 0.2921777367591858\n","Epoch: 270, Loss: 0.32669463753700256\n","Epoch: 271, Loss: 0.2996038496494293\n","Epoch: 272, Loss: 0.2960589826107025\n","Epoch: 273, Loss: 0.32762610912323\n","Epoch: 274, Loss: 0.2795466184616089\n","Epoch: 275, Loss: 0.24685688316822052\n","Epoch: 276, Loss: 0.33243125677108765\n","Epoch: 277, Loss: 0.3481918275356293\n","Epoch: 278, Loss: 0.3097217082977295\n","Epoch: 279, Loss: 0.40503886342048645\n","Epoch: 280, Loss: 0.262876957654953\n","Epoch: 281, Loss: 0.2586022615432739\n","Epoch: 282, Loss: 0.3985053598880768\n","Epoch: 283, Loss: 0.3687564730644226\n","Epoch: 284, Loss: 0.2860085964202881\n","Epoch: 285, Loss: 0.40159329771995544\n","Epoch: 286, Loss: 0.40134698152542114\n","Epoch: 287, Loss: 0.2902577519416809\n","Epoch: 288, Loss: 0.311282217502594\n","Epoch: 289, Loss: 0.3558673560619354\n","Epoch: 290, Loss: 0.3543015718460083\n","Epoch: 291, Loss: 0.28757771849632263\n","Epoch: 292, Loss: 0.3748617470264435\n","Epoch: 293, Loss: 0.30591800808906555\n","Epoch: 294, Loss: 0.30336257815361023\n","Epoch: 295, Loss: 0.3256930410861969\n","Epoch: 296, Loss: 0.32191771268844604\n","Epoch: 297, Loss: 0.3254023492336273\n","Epoch: 298, Loss: 0.2656765878200531\n","Epoch: 299, Loss: 0.264718621969223\n","Epoch: 300, Loss: 0.2518670856952667\n","Epoch: 301, Loss: 0.27654829621315\n","Epoch: 302, Loss: 0.31722643971443176\n","Epoch: 303, Loss: 0.31929463148117065\n","Epoch: 304, Loss: 0.30216506123542786\n","Epoch: 305, Loss: 0.2668825387954712\n","Epoch: 306, Loss: 0.26872938871383667\n","Epoch: 307, Loss: 0.32341891527175903\n","Epoch: 308, Loss: 0.34693652391433716\n","Epoch: 309, Loss: 0.30567818880081177\n","Epoch: 310, Loss: 0.291359543800354\n","Epoch: 311, Loss: 0.2515254020690918\n","Epoch: 312, Loss: 0.28522977232933044\n","Epoch: 313, Loss: 0.3139372169971466\n","Epoch: 314, Loss: 0.22252632677555084\n","Epoch: 315, Loss: 0.32514166831970215\n","Epoch: 316, Loss: 0.2931315004825592\n","Epoch: 317, Loss: 0.30848631262779236\n","Epoch: 318, Loss: 0.30889183282852173\n","Epoch: 319, Loss: 0.27959349751472473\n","Epoch: 320, Loss: 0.2970745861530304\n","Epoch: 321, Loss: 0.3209642767906189\n","Epoch: 322, Loss: 0.31409233808517456\n","Epoch: 323, Loss: 0.27948901057243347\n","Epoch: 324, Loss: 0.2885642647743225\n","Epoch: 325, Loss: 0.25858861207962036\n","Epoch: 326, Loss: 0.2962210774421692\n","Epoch: 327, Loss: 0.23299993574619293\n","Epoch: 328, Loss: 0.2582489252090454\n","Epoch: 329, Loss: 0.2829670310020447\n","Epoch: 330, Loss: 0.23822146654129028\n","Epoch: 331, Loss: 0.24678641557693481\n","Epoch: 332, Loss: 0.30480921268463135\n","Epoch: 333, Loss: 0.3390892744064331\n","Epoch: 334, Loss: 0.26717305183410645\n","Epoch: 335, Loss: 0.26674675941467285\n","Epoch: 336, Loss: 0.268614798784256\n","Epoch: 337, Loss: 0.28303366899490356\n","Epoch: 338, Loss: 0.30883756279945374\n","Epoch: 339, Loss: 0.2911438047885895\n","Epoch: 340, Loss: 0.31213250756263733\n","Epoch: 341, Loss: 0.3231799304485321\n","Epoch: 342, Loss: 0.25955817103385925\n","Epoch: 343, Loss: 0.28562259674072266\n","Epoch: 344, Loss: 0.26129651069641113\n","Epoch: 345, Loss: 0.37760403752326965\n","Epoch: 346, Loss: 0.29724305868148804\n","Epoch: 347, Loss: 0.2970617413520813\n","Epoch: 348, Loss: 0.3741271197795868\n","Epoch: 349, Loss: 0.2752508223056793\n","Epoch: 350, Loss: 0.23130756616592407\n","Epoch: 351, Loss: 0.27842655777931213\n","Epoch: 352, Loss: 0.28373628854751587\n","Epoch: 353, Loss: 0.26377686858177185\n","Epoch: 354, Loss: 0.24869368970394135\n","Epoch: 355, Loss: 0.35598745942115784\n","Epoch: 356, Loss: 0.27121907472610474\n","Epoch: 357, Loss: 0.311392217874527\n","Epoch: 358, Loss: 0.31497034430503845\n","Epoch: 359, Loss: 0.30142027139663696\n","Epoch: 360, Loss: 0.3155219256877899\n","Epoch: 361, Loss: 0.28572821617126465\n","Epoch: 362, Loss: 0.31730592250823975\n","Epoch: 363, Loss: 0.2559020519256592\n","Epoch: 364, Loss: 0.25060930848121643\n","Epoch: 365, Loss: 0.32160621881484985\n","Epoch: 366, Loss: 0.2788771688938141\n","Epoch: 367, Loss: 0.23892992734909058\n","Epoch: 368, Loss: 0.2854626774787903\n","Epoch: 369, Loss: 0.30459722876548767\n","Epoch: 370, Loss: 0.29340338706970215\n","Epoch: 371, Loss: 0.26423147320747375\n","Epoch: 372, Loss: 0.34880390763282776\n","Epoch: 373, Loss: 0.30874213576316833\n","Epoch: 374, Loss: 0.21581712365150452\n","Epoch: 375, Loss: 0.28414762020111084\n","Epoch: 376, Loss: 0.22561736404895782\n","Epoch: 377, Loss: 0.23491892218589783\n","Epoch: 378, Loss: 0.2592940032482147\n","Epoch: 379, Loss: 0.22858455777168274\n","Epoch: 380, Loss: 0.24511072039604187\n","Epoch: 381, Loss: 0.2998148798942566\n","Epoch: 382, Loss: 0.2671636641025543\n","Epoch: 383, Loss: 0.2900087237358093\n","Epoch: 384, Loss: 0.20638220012187958\n","Epoch: 385, Loss: 0.3389185965061188\n","Epoch: 386, Loss: 0.2944090962409973\n","Epoch: 387, Loss: 0.23243768513202667\n","Epoch: 388, Loss: 0.26998209953308105\n","Epoch: 389, Loss: 0.307791531085968\n","Epoch: 390, Loss: 0.33941856026649475\n","Epoch: 391, Loss: 0.2666170597076416\n","Epoch: 392, Loss: 0.28082987666130066\n","Epoch: 393, Loss: 0.24825727939605713\n","Epoch: 394, Loss: 0.3388318717479706\n","Epoch: 395, Loss: 0.3077491819858551\n","Epoch: 396, Loss: 0.22484637796878815\n","Epoch: 397, Loss: 0.2686240077018738\n","Epoch: 398, Loss: 0.2958616018295288\n","Epoch: 399, Loss: 0.23493966460227966\n","Epoch: 400, Loss: 0.23043127357959747\n","Epoch: 401, Loss: 0.277009516954422\n","Epoch: 402, Loss: 0.2565918564796448\n","Epoch: 403, Loss: 0.33364441990852356\n","Epoch: 404, Loss: 0.2215985357761383\n","Epoch: 405, Loss: 0.2659306228160858\n","Epoch: 406, Loss: 0.26295045018196106\n","Epoch: 407, Loss: 0.2412933111190796\n","Epoch: 408, Loss: 0.24881227314472198\n","Epoch: 409, Loss: 0.2618725001811981\n","Epoch: 410, Loss: 0.22161638736724854\n","Epoch: 411, Loss: 0.2212582230567932\n","Epoch: 412, Loss: 0.24238690733909607\n","Epoch: 413, Loss: 0.23385420441627502\n","Epoch: 414, Loss: 0.2866353988647461\n","Epoch: 415, Loss: 0.27659329771995544\n","Epoch: 416, Loss: 0.225197434425354\n","Epoch: 417, Loss: 0.32594412565231323\n","Epoch: 418, Loss: 0.23189599812030792\n","Epoch: 419, Loss: 0.33650192618370056\n","Epoch: 420, Loss: 0.3030722439289093\n","Epoch: 421, Loss: 0.25026407837867737\n","Epoch: 422, Loss: 0.27075350284576416\n","Epoch: 423, Loss: 0.280152291059494\n","Epoch: 424, Loss: 0.23061266541481018\n","Epoch: 425, Loss: 0.26919037103652954\n","Epoch: 426, Loss: 0.2803199887275696\n","Epoch: 427, Loss: 0.22908161580562592\n","Epoch: 428, Loss: 0.2648308277130127\n","Epoch: 429, Loss: 0.3319477140903473\n","Epoch: 430, Loss: 0.2770298421382904\n","Epoch: 431, Loss: 0.28326642513275146\n","Epoch: 432, Loss: 0.3087007403373718\n","Epoch: 433, Loss: 0.224721297621727\n","Epoch: 434, Loss: 0.2718866467475891\n","Epoch: 435, Loss: 0.2312767654657364\n","Epoch: 436, Loss: 0.21138843894004822\n","Epoch: 437, Loss: 0.24630410969257355\n","Epoch: 438, Loss: 0.2644351124763489\n","Epoch: 439, Loss: 0.26106107234954834\n","Epoch: 440, Loss: 0.2526813745498657\n","Epoch: 441, Loss: 0.2575148642063141\n","Epoch: 442, Loss: 0.24386131763458252\n","Epoch: 443, Loss: 0.23208841681480408\n","Epoch: 444, Loss: 0.18788915872573853\n","Epoch: 445, Loss: 0.26153284311294556\n","Epoch: 446, Loss: 0.25027143955230713\n","Epoch: 447, Loss: 0.2825656235218048\n","Epoch: 448, Loss: 0.23390048742294312\n","Epoch: 449, Loss: 0.20010586082935333\n","Epoch: 450, Loss: 0.2497832477092743\n","Epoch: 451, Loss: 0.2927432954311371\n","Epoch: 452, Loss: 0.2209012806415558\n","Epoch: 453, Loss: 0.25308749079704285\n","Epoch: 454, Loss: 0.2848275601863861\n","Epoch: 455, Loss: 0.2537249028682709\n","Epoch: 456, Loss: 0.3155016601085663\n","Epoch: 457, Loss: 0.3282102346420288\n","Epoch: 458, Loss: 0.23989062011241913\n","Epoch: 459, Loss: 0.30180418491363525\n","Epoch: 460, Loss: 0.2296471744775772\n","Epoch: 461, Loss: 0.2329687476158142\n","Epoch: 462, Loss: 0.21906425058841705\n","Epoch: 463, Loss: 0.2710355520248413\n","Epoch: 464, Loss: 0.27985984086990356\n","Epoch: 465, Loss: 0.26034754514694214\n","Epoch: 466, Loss: 0.3269351124763489\n","Epoch: 467, Loss: 0.21647706627845764\n","Epoch: 468, Loss: 0.24509333074092865\n","Epoch: 469, Loss: 0.25504785776138306\n","Epoch: 470, Loss: 0.24944478273391724\n","Epoch: 471, Loss: 0.23005126416683197\n","Epoch: 472, Loss: 0.22157573699951172\n","Epoch: 473, Loss: 0.24492354691028595\n","Epoch: 474, Loss: 0.2628501057624817\n","Epoch: 475, Loss: 0.22913795709609985\n","Epoch: 476, Loss: 0.22873014211654663\n","Epoch: 477, Loss: 0.2480856329202652\n","Epoch: 478, Loss: 0.21324612200260162\n","Epoch: 479, Loss: 0.27204811573028564\n","Epoch: 480, Loss: 0.22072575986385345\n","Epoch: 481, Loss: 0.21503441035747528\n","Epoch: 482, Loss: 0.22492724657058716\n","Epoch: 483, Loss: 0.30736270546913147\n","Epoch: 484, Loss: 0.208298459649086\n","Epoch: 485, Loss: 0.18283332884311676\n","Epoch: 486, Loss: 0.27745163440704346\n","Epoch: 487, Loss: 0.26150646805763245\n","Epoch: 488, Loss: 0.2494027316570282\n","Epoch: 489, Loss: 0.27762606739997864\n","Epoch: 490, Loss: 0.22263717651367188\n","Epoch: 491, Loss: 0.23977969586849213\n","Epoch: 492, Loss: 0.2332782745361328\n","Epoch: 493, Loss: 0.21907654404640198\n","Epoch: 494, Loss: 0.32582470774650574\n","Epoch: 495, Loss: 0.23451605439186096\n","Epoch: 496, Loss: 0.2776818573474884\n","Epoch: 497, Loss: 0.24814550578594208\n","Epoch: 498, Loss: 0.23444585502147675\n","Epoch: 499, Loss: 0.2510978579521179\n","Epoch: 500, Loss: 0.28688427805900574\n","Epoch: 501, Loss: 0.23245513439178467\n","Epoch: 502, Loss: 0.21809504926204681\n","Epoch: 503, Loss: 0.21837759017944336\n","Epoch: 504, Loss: 0.1823907494544983\n","Epoch: 505, Loss: 0.19330152869224548\n","Epoch: 506, Loss: 0.24146126210689545\n","Epoch: 507, Loss: 0.24844743311405182\n","Epoch: 508, Loss: 0.2411135733127594\n","Epoch: 509, Loss: 0.234286829829216\n","Epoch: 510, Loss: 0.22303889691829681\n","Epoch: 511, Loss: 0.20483140647411346\n","Epoch: 512, Loss: 0.25536319613456726\n","Epoch: 513, Loss: 0.17269325256347656\n","Epoch: 514, Loss: 0.2195950150489807\n","Epoch: 515, Loss: 0.2275812178850174\n","Epoch: 516, Loss: 0.24498111009597778\n","Epoch: 517, Loss: 0.22282089293003082\n","Epoch: 518, Loss: 0.24811872839927673\n","Epoch: 519, Loss: 0.21690165996551514\n","Epoch: 520, Loss: 0.23824016749858856\n","Epoch: 521, Loss: 0.2398357391357422\n","Epoch: 522, Loss: 0.27171894907951355\n","Epoch: 523, Loss: 0.23515263199806213\n","Epoch: 524, Loss: 0.27134978771209717\n","Epoch: 525, Loss: 0.23973886668682098\n","Epoch: 526, Loss: 0.21171677112579346\n","Epoch: 527, Loss: 0.2715561091899872\n","Epoch: 528, Loss: 0.26290762424468994\n","Epoch: 529, Loss: 0.1803128570318222\n","Epoch: 530, Loss: 0.21246610581874847\n","Epoch: 531, Loss: 0.24629567563533783\n","Epoch: 532, Loss: 0.2528710663318634\n","Epoch: 533, Loss: 0.22888074815273285\n","Epoch: 534, Loss: 0.18487213551998138\n","Epoch: 535, Loss: 0.22906175255775452\n","Epoch: 536, Loss: 0.2632196545600891\n","Epoch: 537, Loss: 0.21164055168628693\n","Epoch: 538, Loss: 0.25908583402633667\n","Epoch: 539, Loss: 0.2341979742050171\n","Epoch: 540, Loss: 0.23550455272197723\n","Epoch: 541, Loss: 0.24296054244041443\n","Epoch: 542, Loss: 0.25541090965270996\n","Epoch: 543, Loss: 0.22623005509376526\n","Epoch: 544, Loss: 0.25852036476135254\n","Epoch: 545, Loss: 0.23670822381973267\n","Epoch: 546, Loss: 0.26983681321144104\n","Epoch: 547, Loss: 0.23631225526332855\n","Epoch: 548, Loss: 0.26214250922203064\n","Epoch: 549, Loss: 0.2185247838497162\n","Epoch: 550, Loss: 0.17988254129886627\n","Epoch: 551, Loss: 0.20630325376987457\n","Epoch: 552, Loss: 0.1721070408821106\n","Epoch: 553, Loss: 0.22354312241077423\n","Epoch: 554, Loss: 0.22325031459331512\n","Epoch: 555, Loss: 0.202469140291214\n","Epoch: 556, Loss: 0.1965029090642929\n","Epoch: 557, Loss: 0.1939190924167633\n","Epoch: 558, Loss: 0.19291096925735474\n","Epoch: 559, Loss: 0.2057623267173767\n","Epoch: 560, Loss: 0.24561187624931335\n","Epoch: 561, Loss: 0.2624605894088745\n","Epoch: 562, Loss: 0.22642230987548828\n","Epoch: 563, Loss: 0.23267635703086853\n","Epoch: 564, Loss: 0.21754884719848633\n","Epoch: 565, Loss: 0.23142383992671967\n","Epoch: 566, Loss: 0.18560077250003815\n","Epoch: 567, Loss: 0.27964702248573303\n","Epoch: 568, Loss: 0.17558665573596954\n","Epoch: 569, Loss: 0.2248152196407318\n","Epoch: 570, Loss: 0.2162047028541565\n","Epoch: 571, Loss: 0.20079344511032104\n","Epoch: 572, Loss: 0.23678253591060638\n","Epoch: 573, Loss: 0.23697003722190857\n","Epoch: 574, Loss: 0.18758539855480194\n","Epoch: 575, Loss: 0.2329147458076477\n","Epoch: 576, Loss: 0.19781547784805298\n","Epoch: 577, Loss: 0.23896025121212006\n","Epoch: 578, Loss: 0.22964322566986084\n","Epoch: 579, Loss: 0.24840237200260162\n","Epoch: 580, Loss: 0.20376114547252655\n","Epoch: 581, Loss: 0.25461137294769287\n","Epoch: 582, Loss: 0.24437427520751953\n","Epoch: 583, Loss: 0.19282513856887817\n","Epoch: 584, Loss: 0.2017659991979599\n","Epoch: 585, Loss: 0.20993384718894958\n","Epoch: 586, Loss: 0.18869639933109283\n","Epoch: 587, Loss: 0.2518977224826813\n","Epoch: 588, Loss: 0.2707645297050476\n","Epoch: 589, Loss: 0.2170901894569397\n","Epoch: 590, Loss: 0.2587546408176422\n","Epoch: 591, Loss: 0.1797102987766266\n","Epoch: 592, Loss: 0.2297438383102417\n","Epoch: 593, Loss: 0.19335685670375824\n","Epoch: 594, Loss: 0.20920884609222412\n","Epoch: 595, Loss: 0.26628777384757996\n","Epoch: 596, Loss: 0.1918032467365265\n","Epoch: 597, Loss: 0.209761843085289\n","Epoch: 598, Loss: 0.21251127123832703\n","Epoch: 599, Loss: 0.20405955612659454\n","Epoch: 600, Loss: 0.22007961571216583\n","Epoch: 601, Loss: 0.22770507633686066\n","Epoch: 602, Loss: 0.23991554975509644\n","Epoch: 603, Loss: 0.2581586539745331\n","Epoch: 604, Loss: 0.21001848578453064\n","Epoch: 605, Loss: 0.26059412956237793\n","Epoch: 606, Loss: 0.250354140996933\n","Epoch: 607, Loss: 0.19307830929756165\n","Epoch: 608, Loss: 0.270984947681427\n","Epoch: 609, Loss: 0.281401127576828\n","Epoch: 610, Loss: 0.2018214762210846\n","Epoch: 611, Loss: 0.2105904072523117\n","Epoch: 612, Loss: 0.21104015409946442\n","Epoch: 613, Loss: 0.2372334599494934\n","Epoch: 614, Loss: 0.20091325044631958\n","Epoch: 615, Loss: 0.224466934800148\n","Epoch: 616, Loss: 0.22388368844985962\n","Epoch: 617, Loss: 0.2876509726047516\n","Epoch: 618, Loss: 0.17336449027061462\n","Epoch: 619, Loss: 0.2713928818702698\n","Epoch: 620, Loss: 0.2872632145881653\n","Epoch: 621, Loss: 0.21209664642810822\n","Epoch: 622, Loss: 0.24903583526611328\n","Epoch: 623, Loss: 0.19811399281024933\n","Epoch: 624, Loss: 0.2443227618932724\n","Epoch: 625, Loss: 0.23278795182704926\n","Epoch: 626, Loss: 0.2235584259033203\n","Epoch: 627, Loss: 0.24867604672908783\n","Epoch: 628, Loss: 0.22190140187740326\n","Epoch: 629, Loss: 0.2109348326921463\n","Epoch: 630, Loss: 0.2489258050918579\n","Epoch: 631, Loss: 0.27653059363365173\n","Epoch: 632, Loss: 0.18298599123954773\n","Epoch: 633, Loss: 0.2466883808374405\n","Epoch: 634, Loss: 0.17298775911331177\n","Epoch: 635, Loss: 0.25050997734069824\n","Epoch: 636, Loss: 0.20034632086753845\n","Epoch: 637, Loss: 0.23846592009067535\n","Epoch: 638, Loss: 0.28119710087776184\n","Epoch: 639, Loss: 0.2577512264251709\n","Epoch: 640, Loss: 0.1966465562582016\n","Epoch: 641, Loss: 0.19905833899974823\n","Epoch: 642, Loss: 0.22386892139911652\n","Epoch: 643, Loss: 0.2545977830886841\n","Epoch: 644, Loss: 0.2674100995063782\n","Epoch: 645, Loss: 0.1929076910018921\n","Epoch: 646, Loss: 0.17403778433799744\n","Epoch: 647, Loss: 0.21608808636665344\n","Epoch: 648, Loss: 0.20327436923980713\n","Epoch: 649, Loss: 0.19728516042232513\n","Epoch: 650, Loss: 0.17971105873584747\n","Epoch: 651, Loss: 0.22506500780582428\n","Epoch: 652, Loss: 0.23215723037719727\n","Epoch: 653, Loss: 0.209706112742424\n","Epoch: 654, Loss: 0.20637516677379608\n","Epoch: 655, Loss: 0.20945918560028076\n","Epoch: 656, Loss: 0.22042709589004517\n","Epoch: 657, Loss: 0.205204576253891\n","Epoch: 658, Loss: 0.2489645928144455\n","Epoch: 659, Loss: 0.209156334400177\n","Epoch: 660, Loss: 0.17286212742328644\n","Epoch: 661, Loss: 0.1957763284444809\n","Epoch: 662, Loss: 0.21289069950580597\n","Epoch: 663, Loss: 0.19615451991558075\n","Epoch: 664, Loss: 0.15692941844463348\n","Epoch: 665, Loss: 0.18298038840293884\n","Epoch: 666, Loss: 0.22876542806625366\n","Epoch: 667, Loss: 0.1870589405298233\n","Epoch: 668, Loss: 0.2105824500322342\n","Epoch: 669, Loss: 0.18837948143482208\n","Epoch: 670, Loss: 0.2306000292301178\n","Epoch: 671, Loss: 0.2050899863243103\n","Epoch: 672, Loss: 0.20845840871334076\n","Epoch: 673, Loss: 0.23508453369140625\n","Epoch: 674, Loss: 0.19742347300052643\n","Epoch: 675, Loss: 0.19291864335536957\n","Epoch: 676, Loss: 0.2116040736436844\n","Epoch: 677, Loss: 0.2353854924440384\n","Epoch: 678, Loss: 0.18954256176948547\n","Epoch: 679, Loss: 0.2201257199048996\n","Epoch: 680, Loss: 0.2142731100320816\n","Epoch: 681, Loss: 0.1897774338722229\n","Epoch: 682, Loss: 0.15373672544956207\n","Epoch: 683, Loss: 0.20519278943538666\n","Epoch: 684, Loss: 0.21965451538562775\n","Epoch: 685, Loss: 0.19168923795223236\n","Epoch: 686, Loss: 0.18287424743175507\n","Epoch: 687, Loss: 0.18037079274654388\n","Epoch: 688, Loss: 0.20573246479034424\n","Epoch: 689, Loss: 0.21614672243595123\n","Epoch: 690, Loss: 0.20043794810771942\n","Epoch: 691, Loss: 0.17732082307338715\n","Epoch: 692, Loss: 0.19492149353027344\n","Epoch: 693, Loss: 0.1653842329978943\n","Epoch: 694, Loss: 0.1932244449853897\n","Epoch: 695, Loss: 0.1816881000995636\n","Epoch: 696, Loss: 0.18449415266513824\n","Epoch: 697, Loss: 0.16198115050792694\n","Epoch: 698, Loss: 0.19050711393356323\n","Epoch: 699, Loss: 0.16146372258663177\n","Epoch: 700, Loss: 0.3271494209766388\n","Epoch: 701, Loss: 0.2784000635147095\n","Epoch: 702, Loss: 0.2221335768699646\n","Epoch: 703, Loss: 0.29525309801101685\n","Epoch: 704, Loss: 0.17310279607772827\n","Epoch: 705, Loss: 0.2525101602077484\n","Epoch: 706, Loss: 0.18682056665420532\n","Epoch: 707, Loss: 0.22507360577583313\n","Epoch: 708, Loss: 0.19533973932266235\n","Epoch: 709, Loss: 0.2188088297843933\n","Epoch: 710, Loss: 0.29248473048210144\n","Epoch: 711, Loss: 0.19580572843551636\n","Epoch: 712, Loss: 0.231561541557312\n","Epoch: 713, Loss: 0.2514594495296478\n","Epoch: 714, Loss: 0.1894570291042328\n","Epoch: 715, Loss: 0.1798401027917862\n","Epoch: 716, Loss: 0.18739505112171173\n","Epoch: 717, Loss: 0.1869245320558548\n","Epoch: 718, Loss: 0.18821312487125397\n","Epoch: 719, Loss: 0.24211350083351135\n","Epoch: 720, Loss: 0.17586162686347961\n","Epoch: 721, Loss: 0.2570742070674896\n","Epoch: 722, Loss: 0.23761025071144104\n","Epoch: 723, Loss: 0.19977973401546478\n","Epoch: 724, Loss: 0.1843428760766983\n","Epoch: 725, Loss: 0.17957042157649994\n","Epoch: 726, Loss: 0.23115192353725433\n","Epoch: 727, Loss: 0.2013128101825714\n","Epoch: 728, Loss: 0.14541245996952057\n","Epoch: 729, Loss: 0.25114521384239197\n","Epoch: 730, Loss: 0.1987113058567047\n","Epoch: 731, Loss: 0.236358642578125\n","Epoch: 732, Loss: 0.1900741159915924\n","Epoch: 733, Loss: 0.20974597334861755\n","Epoch: 734, Loss: 0.22145086526870728\n","Epoch: 735, Loss: 0.21200531721115112\n","Epoch: 736, Loss: 0.20670568943023682\n","Epoch: 737, Loss: 0.1841963827610016\n","Epoch: 738, Loss: 0.18009088933467865\n","Epoch: 739, Loss: 0.1745723932981491\n","Epoch: 740, Loss: 0.13694772124290466\n","Epoch: 741, Loss: 0.1610201597213745\n","Epoch: 742, Loss: 0.2257627695798874\n","Epoch: 743, Loss: 0.17877593636512756\n","Epoch: 744, Loss: 0.16218514740467072\n","Epoch: 745, Loss: 0.21700650453567505\n","Epoch: 746, Loss: 0.16856175661087036\n","Epoch: 747, Loss: 0.18024080991744995\n","Epoch: 748, Loss: 0.20814035832881927\n","Epoch: 749, Loss: 0.17956224083900452\n","Epoch: 750, Loss: 0.17208519577980042\n","Epoch: 751, Loss: 0.18863968551158905\n","Epoch: 752, Loss: 0.2028091996908188\n","Epoch: 753, Loss: 0.17761893570423126\n","Epoch: 754, Loss: 0.2413438856601715\n","Epoch: 755, Loss: 0.16427917778491974\n","Epoch: 756, Loss: 0.17797733843326569\n","Epoch: 757, Loss: 0.18501095473766327\n","Epoch: 758, Loss: 0.18014246225357056\n","Epoch: 759, Loss: 0.2302599400281906\n","Epoch: 760, Loss: 0.17611266672611237\n","Epoch: 761, Loss: 0.23984447121620178\n","Epoch: 762, Loss: 0.18627464771270752\n","Epoch: 763, Loss: 0.24575336277484894\n","Epoch: 764, Loss: 0.19894197583198547\n","Epoch: 765, Loss: 0.15183156728744507\n","Epoch: 766, Loss: 0.2308337390422821\n","Epoch: 767, Loss: 0.17656420171260834\n","Epoch: 768, Loss: 0.20333629846572876\n","Epoch: 769, Loss: 0.17414703965187073\n","Epoch: 770, Loss: 0.2553057372570038\n","Epoch: 771, Loss: 0.25494149327278137\n","Epoch: 772, Loss: 0.21687665581703186\n","Epoch: 773, Loss: 0.20119474828243256\n","Epoch: 774, Loss: 0.18541301786899567\n","Epoch: 775, Loss: 0.22868703305721283\n","Epoch: 776, Loss: 0.1583000123500824\n","Epoch: 777, Loss: 0.21048694849014282\n","Epoch: 778, Loss: 0.19030708074569702\n","Epoch: 779, Loss: 0.1519852578639984\n","Epoch: 780, Loss: 0.20015084743499756\n","Epoch: 781, Loss: 0.20120611786842346\n","Epoch: 782, Loss: 0.17634788155555725\n","Epoch: 783, Loss: 0.14535240828990936\n","Epoch: 784, Loss: 0.23541022837162018\n","Epoch: 785, Loss: 0.24696743488311768\n","Epoch: 786, Loss: 0.21384519338607788\n","Epoch: 787, Loss: 0.2182798981666565\n","Epoch: 788, Loss: 0.1707974374294281\n","Epoch: 789, Loss: 0.21560606360435486\n","Epoch: 790, Loss: 0.20746532082557678\n","Epoch: 791, Loss: 0.2511584162712097\n","Epoch: 792, Loss: 0.20796427130699158\n","Epoch: 793, Loss: 0.23664602637290955\n","Epoch: 794, Loss: 0.33059215545654297\n","Epoch: 795, Loss: 0.20439252257347107\n","Epoch: 796, Loss: 0.2737863063812256\n","Epoch: 797, Loss: 0.26067349314689636\n","Epoch: 798, Loss: 0.1806233823299408\n","Epoch: 799, Loss: 0.2722975015640259\n","Epoch: 800, Loss: 0.20511206984519958\n","Epoch: 801, Loss: 0.17736849188804626\n","Epoch: 802, Loss: 0.24434874951839447\n","Epoch: 803, Loss: 0.14931288361549377\n","Epoch: 804, Loss: 0.22436341643333435\n","Epoch: 805, Loss: 0.2222185581922531\n","Epoch: 806, Loss: 0.21977704763412476\n","Epoch: 807, Loss: 0.18645456433296204\n","Epoch: 808, Loss: 0.17406484484672546\n","Epoch: 809, Loss: 0.17875558137893677\n","Epoch: 810, Loss: 0.16375461220741272\n","Epoch: 811, Loss: 0.17971351742744446\n","Epoch: 812, Loss: 0.18548345565795898\n","Epoch: 813, Loss: 0.16509422659873962\n","Epoch: 814, Loss: 0.21110951900482178\n","Epoch: 815, Loss: 0.141059011220932\n","Epoch: 816, Loss: 0.19489681720733643\n","Epoch: 817, Loss: 0.21103429794311523\n","Epoch: 818, Loss: 0.19358722865581512\n","Epoch: 819, Loss: 0.19599001109600067\n","Epoch: 820, Loss: 0.17458166182041168\n","Epoch: 821, Loss: 0.22252534329891205\n","Epoch: 822, Loss: 0.17617839574813843\n","Epoch: 823, Loss: 0.19624873995780945\n","Epoch: 824, Loss: 0.196846604347229\n","Epoch: 825, Loss: 0.17654964327812195\n","Epoch: 826, Loss: 0.19745716452598572\n","Epoch: 827, Loss: 0.1849546879529953\n","Epoch: 828, Loss: 0.18138928711414337\n","Epoch: 829, Loss: 0.18350549042224884\n","Epoch: 830, Loss: 0.1990935504436493\n","Epoch: 831, Loss: 0.22348174452781677\n","Epoch: 832, Loss: 0.17954234778881073\n","Epoch: 833, Loss: 0.14639413356781006\n","Epoch: 834, Loss: 0.2068316489458084\n","Epoch: 835, Loss: 0.17734012007713318\n","Epoch: 836, Loss: 0.16904215514659882\n","Epoch: 837, Loss: 0.1726796180009842\n","Epoch: 838, Loss: 0.22070294618606567\n","Epoch: 839, Loss: 0.1863003373146057\n","Epoch: 840, Loss: 0.1905607432126999\n","Epoch: 841, Loss: 0.18362073600292206\n","Epoch: 842, Loss: 0.18595442175865173\n","Epoch: 843, Loss: 0.18369121849536896\n","Epoch: 844, Loss: 0.20977765321731567\n","Epoch: 845, Loss: 0.14969269931316376\n","Epoch: 846, Loss: 0.1661057472229004\n","Epoch: 847, Loss: 0.15899398922920227\n","Epoch: 848, Loss: 0.14020918309688568\n","Epoch: 849, Loss: 0.17458955943584442\n","Epoch: 850, Loss: 0.17800138890743256\n","Epoch: 851, Loss: 0.16979078948497772\n","Epoch: 852, Loss: 0.18675681948661804\n","Epoch: 853, Loss: 0.17614924907684326\n","Epoch: 854, Loss: 0.19241899251937866\n","Epoch: 855, Loss: 0.2002367377281189\n","Epoch: 856, Loss: 0.16546741127967834\n","Epoch: 857, Loss: 0.16946107149124146\n","Epoch: 858, Loss: 0.1768127828836441\n","Epoch: 859, Loss: 0.14267273247241974\n","Epoch: 860, Loss: 0.19084542989730835\n","Epoch: 861, Loss: 0.1579408049583435\n","Epoch: 862, Loss: 0.18748174607753754\n","Epoch: 863, Loss: 0.1713506132364273\n","Epoch: 864, Loss: 0.17238406836986542\n","Epoch: 865, Loss: 0.15345406532287598\n","Epoch: 866, Loss: 0.196968212723732\n","Epoch: 867, Loss: 0.24305690824985504\n","Epoch: 868, Loss: 0.21858318150043488\n","Epoch: 869, Loss: 0.26496535539627075\n","Epoch: 870, Loss: 0.18436016142368317\n","Epoch: 871, Loss: 0.20609883964061737\n","Epoch: 872, Loss: 0.16742916405200958\n","Epoch: 873, Loss: 0.17367494106292725\n","Epoch: 874, Loss: 0.13814222812652588\n","Epoch: 875, Loss: 0.1466841846704483\n","Epoch: 876, Loss: 0.1672748327255249\n","Epoch: 877, Loss: 0.18036508560180664\n","Epoch: 878, Loss: 0.2070094347000122\n","Epoch: 879, Loss: 0.17212672531604767\n","Epoch: 880, Loss: 0.21073389053344727\n","Epoch: 881, Loss: 0.1558849811553955\n","Epoch: 882, Loss: 0.21529120206832886\n","Epoch: 883, Loss: 0.1839584857225418\n","Epoch: 884, Loss: 0.14708469808101654\n","Epoch: 885, Loss: 0.19966767728328705\n","Epoch: 886, Loss: 0.17930598556995392\n","Epoch: 887, Loss: 0.22076553106307983\n","Epoch: 888, Loss: 0.21005235612392426\n","Epoch: 889, Loss: 0.22511300444602966\n","Epoch: 890, Loss: 0.23183786869049072\n","Epoch: 891, Loss: 0.1821340024471283\n","Epoch: 892, Loss: 0.3029443919658661\n","Epoch: 893, Loss: 0.24375811219215393\n","Epoch: 894, Loss: 0.21777519583702087\n","Epoch: 895, Loss: 0.29162004590034485\n","Epoch: 896, Loss: 0.1561952829360962\n","Epoch: 897, Loss: 0.19793570041656494\n","Epoch: 898, Loss: 0.2189873903989792\n","Epoch: 899, Loss: 0.17331993579864502\n","Epoch: 900, Loss: 0.22438178956508636\n","Epoch: 901, Loss: 0.17431612312793732\n","Epoch: 902, Loss: 0.20140968263149261\n","Epoch: 903, Loss: 0.18592678010463715\n","Epoch: 904, Loss: 0.30922746658325195\n","Epoch: 905, Loss: 0.24198701977729797\n","Epoch: 906, Loss: 0.2100469022989273\n","Epoch: 907, Loss: 0.2271202653646469\n","Epoch: 908, Loss: 0.175122931599617\n","Epoch: 909, Loss: 0.20271119475364685\n","Epoch: 910, Loss: 0.1689043641090393\n","Epoch: 911, Loss: 0.15800659358501434\n","Epoch: 912, Loss: 0.19345319271087646\n","Epoch: 913, Loss: 0.17786845564842224\n","Epoch: 914, Loss: 0.23151813447475433\n","Epoch: 915, Loss: 0.19610995054244995\n","Epoch: 916, Loss: 0.20275431871414185\n","Epoch: 917, Loss: 0.20672014355659485\n","Epoch: 918, Loss: 0.16975811123847961\n","Epoch: 919, Loss: 0.215712770819664\n","Epoch: 920, Loss: 0.16020555794239044\n","Epoch: 921, Loss: 0.16363269090652466\n","Epoch: 922, Loss: 0.15087586641311646\n","Epoch: 923, Loss: 0.15470220148563385\n","Epoch: 924, Loss: 0.1875227987766266\n","Epoch: 925, Loss: 0.18548119068145752\n","Epoch: 926, Loss: 0.24531437456607819\n","Epoch: 927, Loss: 0.19541412591934204\n","Epoch: 928, Loss: 0.17241926491260529\n","Epoch: 929, Loss: 0.27266350388526917\n","Epoch: 930, Loss: 0.23968586325645447\n","Epoch: 931, Loss: 0.19294896721839905\n","Epoch: 932, Loss: 0.21146638691425323\n","Epoch: 933, Loss: 0.18960119783878326\n","Epoch: 934, Loss: 0.1914696842432022\n","Epoch: 935, Loss: 0.14261044561862946\n","Epoch: 936, Loss: 0.1653640866279602\n","Epoch: 937, Loss: 0.20333260297775269\n","Epoch: 938, Loss: 0.15300405025482178\n","Epoch: 939, Loss: 0.22171571850776672\n","Epoch: 940, Loss: 0.18798547983169556\n","Epoch: 941, Loss: 0.18122495710849762\n","Epoch: 942, Loss: 0.16750864684581757\n","Epoch: 943, Loss: 0.19021636247634888\n","Epoch: 944, Loss: 0.1525816172361374\n","Epoch: 945, Loss: 0.17808906733989716\n","Epoch: 946, Loss: 0.18187420070171356\n","Epoch: 947, Loss: 0.1529751867055893\n","Epoch: 948, Loss: 0.1836739033460617\n","Epoch: 949, Loss: 0.1580193191766739\n","Epoch: 950, Loss: 0.1571561098098755\n","Epoch: 951, Loss: 0.16494445502758026\n","Epoch: 952, Loss: 0.16536064445972443\n","Epoch: 953, Loss: 0.12521930038928986\n","Epoch: 954, Loss: 0.17411132156848907\n","Epoch: 955, Loss: 0.1918526291847229\n","Epoch: 956, Loss: 0.1859412044286728\n","Epoch: 957, Loss: 0.19915124773979187\n","Epoch: 958, Loss: 0.18918685615062714\n","Epoch: 959, Loss: 0.16224096715450287\n","Epoch: 960, Loss: 0.17823003232479095\n","Epoch: 961, Loss: 0.1494663506746292\n","Epoch: 962, Loss: 0.1558675765991211\n","Epoch: 963, Loss: 0.17031173408031464\n","Epoch: 964, Loss: 0.15511730313301086\n","Epoch: 965, Loss: 0.14990131556987762\n","Epoch: 966, Loss: 0.16453339159488678\n","Epoch: 967, Loss: 0.13084788620471954\n","Epoch: 968, Loss: 0.1787365972995758\n","Epoch: 969, Loss: 0.18173716962337494\n","Epoch: 970, Loss: 0.1729387640953064\n","Epoch: 971, Loss: 0.20757032930850983\n","Epoch: 972, Loss: 0.21821197867393494\n","Epoch: 973, Loss: 0.1477193683385849\n","Epoch: 974, Loss: 0.21911650896072388\n","Epoch: 975, Loss: 0.17320385575294495\n","Epoch: 976, Loss: 0.18643173575401306\n","Epoch: 977, Loss: 0.22031782567501068\n","Epoch: 978, Loss: 0.1900976300239563\n","Epoch: 979, Loss: 0.17114968597888947\n","Epoch: 980, Loss: 0.17058271169662476\n","Epoch: 981, Loss: 0.17736530303955078\n","Epoch: 982, Loss: 0.193191796541214\n","Epoch: 983, Loss: 0.15560570359230042\n","Epoch: 984, Loss: 0.1713166981935501\n"]}],"source":["# from https://discuss.pytorch.org/t/using-transformer-on-timeseries/104759\n","import numpy\n","import math\n","import torch\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","from scipy import stats\n","\n","\n","class PositionalEncoding(nn.Module):\n","\n","    def __init__(self, d_model, dropout=0.1, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0).transpose(0, 1)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.pe[:x.size(0), :]\n","        return self.dropout(x)\n","\n","\n","class SeqTransformer(nn.Module):\n","    def __init__(self, _max_seq_length, _feature_size, _num_layers, _num_heads):\n","        super(SeqTransformer, self).__init__()\n","        self.pos_encoder = PositionalEncoding(_feature_size)\n","        self.embedding = nn.Embedding(_max_seq_length, _feature_size)\n","        self.layers = nn.TransformerEncoderLayer(d_model=_feature_size, nhead=_num_heads)\n","        self.transformer = nn.TransformerEncoder(self.layers, num_layers=_num_layers)\n","        self.decoder = nn.Linear(_feature_size * _max_seq_length, 1)\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        x = self.pos_encoder(x)\n","        x = self.transformer(x)\n","        x = x.view(1, -1)\n","        x = self.decoder(x)\n","        return x\n","\n","\n","MAX_ROWS = 100\n","HALF_ROWS = MAX_ROWS // 2\n","max_seq_length = 111\n","NUM_EPOCHS = 1000\n","num_layers = 2\n","num_heads = 5\n","feature_size = 10\n","torch.manual_seed(1)\n","\n","training_data_fn = \"sim_seq_1_train_sequences2.txt\" #\"/home/jgburk/PycharmProjects/IntroductionToDeepLearning/TF_Data/sim_seq_1_train_sequences.txt\"\n","\n","training_data = numpy.genfromtxt(training_data_fn, delimiter=\"\\t\", dtype=None,\n","                                 skip_header=0, max_rows=MAX_ROWS)\n","training_sequences = list()\n","training_expression = list()\n","mapper = dict([(ord('A'), 1),\n","               (ord('T'), 2),\n","               (ord('C'), 3),\n","               (ord('G'), 4),\n","               (ord('N'), 5)])\n","for row in training_data:\n","    ts = torch.IntTensor([[mapper[x] for x in row[0]]]).transpose(0, 1)\n","    #print(ts)\n","    training_sequences.append(ts)\n","    training_expression.append(torch.FloatTensor([row[1]]))\n","\n","# working through https://stackoverflow.com/questions/56783182/runtimeerror-the-size-of-tensor-a-133-must-match-the-size-of-tensor-b-10-at\n","t_s = torch.nn.utils.rnn.pad_sequence(training_sequences, batch_first=True)\n","t_e = torch.cat(training_expression)\n","\n","model = SeqTransformer(max_seq_length, feature_size, num_layers, num_heads)\n","train_inputs = t_s[0:MAX_ROWS:2, :, :]\n","test_inputs = t_s[1:MAX_ROWS:2, :, :]\n","train_targets = t_e[0:MAX_ROWS:2]\n","test_targets = t_e[1:MAX_ROWS:2]\n","#print(f'inputs.shape: {train_inputs.shape}')\n","\n","loss_fn = nn.L1Loss()\n","optimizer = torch.optim.AdamW(model.parameters())\n","\n","for epoch in range(NUM_EPOCHS):\n","    loss_sum = 0.0\n","    optimizer.zero_grad()\n","    for idx, _input in enumerate(train_inputs):\n","        score = model(_input)\n","        loss = loss_fn(score.view(1, -1), train_targets[idx].view(1, -1))\n","        loss.backward()\n","        loss_sum = loss_sum + torch.abs(loss)\n","    optimizer.step()\n","    print(f'Epoch: {epoch}, Loss: {loss_sum / len(train_inputs)}')\n","\n","pred_l = list()\n","actual_l = list()\n","loss_sq_l = list()\n","for idx, test_input in enumerate(test_inputs):\n","    pred = model(test_input)\n","    pred = pred.item()\n","    actual = test_targets[idx]\n","    actual = actual.item()\n","    loss = pred - actual\n","    print(f'loss[{idx}]: {loss}')\n","    pred_l.append(pred)\n","    actual_l.append(actual)\n","    loss_sq_l.append(abs(loss))\n","\n","psn = stats.pearsonr(x=pred_l, y=actual_l)\n","spn = stats.spearmanr(a=pred_l, b=actual_l)\n","plt.scatter(x=pred_l, y=actual_l, c=loss_sq_l)\n","plt.xlim([min(pred_l + actual_l) - 1, max(pred_l + actual_l) + 1])\n","plt.ylim([min(pred_l + actual_l) - 1, max(pred_l + actual_l) + 1])\n","plt.xlabel(\"Predicted Expression\")\n","plt.ylabel(\"Actual Expression\")\n","plt.suptitle(f'(Pearson + Spearman)/2 = {round((psn[0] + spn.correlation) / 2.0, ndigits=3)}')\n","plt.title(f'mean(P-Values) = {round((psn[1] + spn.pvalue) / 2.0, ndigits=3)}')\n","plt.show()\n","print(max(loss_sq_l))\n","print(f'Pearson r={round(psn[0], ndigits=3)}, P-Value={round(psn[1], ndigits=3)}')\n","print(f'Spearman correlation={round(spn.correlation, ndigits=3)}, P-Value={round(spn.pvalue, ndigits=3)}')"]}]}